{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model's input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import logging\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "from time import strftime, localtime\n",
    "import random\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize for Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Tokenizer4Bert:\n",
    "    def __init__(self, max_seq_len, pretrained_bert_name , max_num_chunks ):\n",
    "        # Load pretrained model/tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_bert_name)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_num_chunks = max_num_chunks\n",
    "\n",
    "    def long_text_to_chunks(self, text):\n",
    "        \"\"\"return an array with shape of [30, max_seq_len], and the element is the token representation of the BERT\"\"\"\n",
    "        import numpy as np\n",
    "        ls_of_tokens = self.tokenizer.tokenize(text)\n",
    "        #print('ls of tokens: {},len:{}'.format(ls_of_tokens, len(ls_of_tokens)))\n",
    "        n = len(ls_of_tokens) // self.max_seq_len\n",
    "        #print('n: {} '.format(n))\n",
    "        res = []\n",
    "        for i in range(self.max_num_chunks):\n",
    "            if i < n:\n",
    "                sub_ls_of_tokens = ls_of_tokens[i*self.max_seq_len:i*self.max_seq_len + self.max_seq_len]\n",
    "            elif i == n:\n",
    "                tmp_len = len(ls_of_tokens[i*self.max_seq_len:])\n",
    "                sub_ls_of_tokens = ls_of_tokens[i*self.max_seq_len:]+['[PAD]']*(self.max_seq_len-tmp_len)\n",
    "            else:\n",
    "                sub_ls_of_tokens = ['[PAD]']*self.max_seq_len\n",
    "            # convert ls of toens to sequence of ids\n",
    "            sub_ls_of_tokens = self.tokenizer.convert_tokens_to_ids(sub_ls_of_tokens)\n",
    "            res.append(sub_ls_of_tokens)\n",
    "        return np.array(res)\n",
    "\n",
    "    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n",
    "        sequence = self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(text))\n",
    "        #print(\"seq: {}, len:{}\".format(sequence,len(sequence)))\n",
    "        if len(sequence) == 0:\n",
    "            sequence = [0]\n",
    "            print('seq:{}'.format(sequence))\n",
    "        if reverse:\n",
    "            sequence = sequence[::-1]\n",
    "            print('seq:{}'.format(sequence))\n",
    "        print(len(self.pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)))\n",
    "\n",
    "        return self.pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)\n",
    "\n",
    "    def pad_and_truncate(self, sequence, maxlen, dtype='int64', padding='post', truncating='post', value=0):\n",
    "        import numpy as np\n",
    "        x = (np.ones(maxlen) * value).astype(dtype)\n",
    "        if truncating == 'pre':\n",
    "            trunc = sequence[-maxlen:]\n",
    "        else:\n",
    "            trunc = sequence[:maxlen]\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if padding == 'post':\n",
    "            x[:len(trunc)] = trunc\n",
    "        else:\n",
    "            x[-len(trunc):] = trunc\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABSADataset(Dataset):\n",
    "    def __init__(self, fname, tokenizer):\n",
    "        fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "        lines = fin.readlines()[:100] # 先看前面50筆 ！！！！！！\n",
    "        fin.close() \n",
    "        print(len(lines))\n",
    "        all_data = []\n",
    "        for i in range(0, len(lines), 2):\n",
    "            text = lines[i].strip()\n",
    "            polarity = lines[i + 1].strip()\n",
    "            # single-sentence classification\n",
    "            text_raw_bert_indices = tokenizer.text_to_sequence(\"[CLS] \" + text)\n",
    "            # documentation classification\n",
    "            text_raw_bert_documents = tokenizer.long_text_to_chunks(\"[CLS] \" + text)\n",
    "            # label\n",
    "            polarity = int(polarity)  # range betwee 0 to num_class -1\n",
    "\n",
    "            data = {\n",
    "                'text_raw_bert_indices': text_raw_bert_indices,\n",
    "                'text_raw_bert_documents': text_raw_bert_documents,\n",
    "                'polarity': polarity,\n",
    "            }\n",
    "            all_data.append(data)\n",
    "\n",
    "        self.data = all_data\n",
    "\n",
    "    def get_dataframe(self, tokenizer):\n",
    "        \"\"\"\n",
    "        Conver dataset into DataFrame(Pandas)\n",
    "        It's only support for bert-based model.\n",
    "        \"\"\"\n",
    "        df = []\n",
    "        columns_name = []\n",
    "        for i in range(len(self.data)):\n",
    "            tmp = []\n",
    "            for k, v in self.data[i].items():\n",
    "                try:\n",
    "                    to_str = \" \".join(tokenizer.tokenizer.convert_ids_to_tokens(v))\n",
    "                    tmp.append(to_str)\n",
    "                except:\n",
    "                    if k == 'aspect_in_text':\n",
    "                        # it's a 1-D tensor wtih shape of (2,), representing the start and end index of the aspect\n",
    "                        v = v.numpy()  # 1-D tensor\n",
    "                        #print (v.shape)\n",
    "                    tmp.append(v)\n",
    "                if i <= 0:\n",
    "                    columns_name.append(k)\n",
    "            df.append(tmp)\n",
    "        df = pd.DataFrame(df,columns=columns_name)\n",
    "        return df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 100\n",
    "pretrained_bert_name = \"bert-base-uncased\"\n",
    "max_num_chunks = 3\n",
    "train_path = 'data/train.txt'\n",
    "test_path = 'data/test.txt'\n",
    "tokenizer = Tokenizer4Bert(max_seq_len, pretrained_bert_name, max_num_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (626 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1117 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (756 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (728 > 512). Running this sequence through BERT will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (735 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (690 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (566 > 512). Running this sequence through BERT will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "40\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "number of traning data 50\n",
      "number of testing data 20\n"
     ]
    }
   ],
   "source": [
    "trainset = ABSADataset(train_path, tokenizer)\n",
    "testset = ABSADataset(test_path, tokenizer)\n",
    "print ('number of traning data', len(trainset))\n",
    "print ('number of testing data', len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = trainset.get_dataframe(tokenizer)\n",
    "test_df = testset.get_dataframe(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 50)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df), len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text_raw_bert_indices      0\n",
       "text_raw_bert_documents    0\n",
       "polarity                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ABSADataset at 0x128bf46d8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters for train\n",
    "learning_rate = 2e-5\n",
    "num_epoch = 10\n",
    "batch_size = 64\n",
    "log_step = 5\n",
    "valset_ratio = 0\n",
    "get_tokenized_result = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters for predict\n",
    "# model_name = 'bert_ssc'\n",
    "pretrained_bert_name = 'bert-base-uncased'\n",
    "dropout = 0.1\n",
    "bert_dim = 768\n",
    "polarities_dim = 2\n",
    "device = None\n",
    "state_dict_path = \"artifacts/bert_ssc_val_acc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "- single sentence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BERT_SSC(nn.Module):\n",
    "    \"\"\"single sentence classification\"\"\"\n",
    "    def __init__(self, bert, opt):\n",
    "        super(BERT_SSC, self).__init__()\n",
    "        # self.squeeze_embedding = SqueezeEmbedding()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(opt.dropout)\n",
    "        self.dense = nn.Linear(opt.bert_dim, opt.polarities_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        text_bert_indices = inputs[0]\n",
    "        _, pooled_output = self.bert(text_bert_indices, output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.dense(pooled_output)\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss CrossEntropyLoss()\n",
      "optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 2e-05\n",
      "    weight_decay: 0.01\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Loss and Optimizer\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "bert = BertModel.from_pretrained(pretrained_bert_name)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "print (\"loss\",criterion)\n",
    "\n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d\n",
    "        \n",
    "opt = {\n",
    "    \"model_name\": 'bert_ssc',\n",
    "    \"device\":device,\n",
    "    \"log_step\": 5,\n",
    "    \"dropout\":0.1,\n",
    "    \"hidden_dim\":768,\n",
    "    \"bert_dim\":768,\n",
    "    \"polarities_dim\":2,\n",
    "    \"learning_rate\":2e-5,\n",
    "    \"l2reg\":0.01,\n",
    "    \"num_epoch\":10,\n",
    "    \"batch_size\":64,\n",
    "    \"optimizer\":torch.optim.Adam,\n",
    "    \"inputs_cols\":['text_raw_bert_indices']\n",
    "}\n",
    "opt = objectview(opt)\n",
    "\n",
    "model = BERT_SSC(bert,opt)\n",
    "_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = opt.optimizer(_params, lr=opt.learning_rate, weight_decay=opt.l2reg)\n",
    "print (\"optimizer\", optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on BERT_SSC in module __main__ object:\n",
      "\n",
      "class BERT_SSC(torch.nn.modules.module.Module)\n",
      " |  single sentence classification\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BERT_SSC\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, bert, opt)\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  forward(self, inputs)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      __dir__() -> list\n",
      " |      default dir() implementation\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.data.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  buffers(self, recurse=True)\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf.data), buf.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self)\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should reimplement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all floating point parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict, strict=True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix='', recurse=True)\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix='', recurse=True)\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse=True)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |      \n",
      " |      .. warning ::\n",
      " |      \n",
      " |          The current implementation will not have the presented behavior\n",
      " |          for complex :class:`Module` that perform many operations.\n",
      " |          In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only\n",
      " |          contain the gradients for a subset of the inputs and outputs.\n",
      " |          For such :class:`Module`, you should use :func:`torch.Tensor.register_hook`\n",
      " |          directly on a specific input or output to get the required gradients.\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  requires_grad_(self, requires_grad=True)\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point desired :attr:`dtype` s. In addition, this method will\n",
      " |      only cast the floating point parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point type of\n",
      " |              the floating point parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training precedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "valset = testset \n",
    "train_data_loader = DataLoader(dataset=trainset, batch_size=opt.batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(dataset=testset, batch_size=opt.batch_size, shuffle=False)\n",
    "val_data_loader = DataLoader(dataset=valset, batch_size=opt.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ABSADataset at 0x13d06e7b8>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_acc_f1(data_loader):\n",
    "    n_correct, n_total = 0, 0\n",
    "    t_targets_all, t_outputs_all = None, None\n",
    "    # switch model to evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for t_batch, t_sample_batched in enumerate(data_loader):\n",
    "            t_inputs = [t_sample_batched[col].to(opt.device) for col in opt.inputs_cols]\n",
    "            t_targets = t_sample_batched['polarity'].to(opt.device)\n",
    "            t_outputs = model(t_inputs)\n",
    "\n",
    "            n_correct += (torch.argmax(t_outputs, -1) == t_targets).sum().item()\n",
    "            n_total += len(t_outputs)\n",
    "\n",
    "            if t_targets_all is None:\n",
    "                t_targets_all = t_targets\n",
    "                t_outputs_all = t_outputs\n",
    "            else:\n",
    "                t_targets_all = torch.cat((t_targets_all, t_targets), dim=0)\n",
    "                t_outputs_all = torch.cat((t_outputs_all, t_outputs), dim=0)\n",
    "\n",
    "    acc = n_correct / n_total\n",
    "    f1 = f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[i for i in range(opt.polarities_dim)], average='macro')\n",
    "    precision = precision_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[i for i in range(opt.polarities_dim)], average='macro')\n",
    "    recall = recall_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[i for i in range(opt.polarities_dim)], average='macro')\n",
    "    return acc, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 0\n",
      "<class 'dict'>\n",
      "====================================================================================================\n",
      "inputs\n",
      "1\n",
      "[tensor([[  101,  2899,  1006,  ..., 11670,  3228,  4855],\n",
      "        [  101,  2899,  1006,  ...,  5290,  1005,  3177],\n",
      "        [  101,  2089,  2343,  ...,  2190,  2240,  2154],\n",
      "        ...,\n",
      "        [  101,  2406,  3220,  ...,  3971,  1024, 11562],\n",
      "        [  101, 20312,  1006,  ...,     0,     0,     0],\n",
      "        [  101,  2343,  8398,  ...,  8398,  2134,  1521]])]\n",
      "====================================================================================================\n",
      "outputs\n",
      "50\n",
      "tensor([[-0.1916, -0.2758],\n",
      "        [-0.1141, -0.1732],\n",
      "        [-0.1051, -0.2890],\n",
      "        [-0.0659, -0.2030],\n",
      "        [ 0.0780, -0.1888],\n",
      "        [ 0.0760, -0.1320],\n",
      "        [-0.1432, -0.0323],\n",
      "        [-0.0042, -0.2424],\n",
      "        [-0.2997,  0.0013],\n",
      "        [-0.2493, -0.1699],\n",
      "        [-0.1241, -0.1292],\n",
      "        [-0.1571, -0.1011],\n",
      "        [-0.3361, -0.1103],\n",
      "        [-0.1150, -0.1287],\n",
      "        [-0.1137, -0.2442],\n",
      "        [ 0.1023, -0.2832],\n",
      "        [ 0.0375, -0.4481],\n",
      "        [ 0.0509,  0.0544],\n",
      "        [-0.1245, -0.2486],\n",
      "        [-0.2471, -0.2110],\n",
      "        [-0.1721, -0.0279],\n",
      "        [-0.2078, -0.1456],\n",
      "        [-0.1250, -0.1251],\n",
      "        [-0.0510, -0.2655],\n",
      "        [ 0.0750, -0.2967],\n",
      "        [-0.1750,  0.4490],\n",
      "        [-0.1997, -0.1825],\n",
      "        [-0.1371, -0.1863],\n",
      "        [-0.1791,  0.1436],\n",
      "        [-0.0393, -0.1763],\n",
      "        [ 0.0722, -0.4246],\n",
      "        [-0.1238, -0.3485],\n",
      "        [-0.1916, -0.1350],\n",
      "        [-0.0796, -0.2639],\n",
      "        [-0.0813,  0.0680],\n",
      "        [-0.2357, -0.1087],\n",
      "        [ 0.0429,  0.0844],\n",
      "        [-0.1180, -0.2854],\n",
      "        [-0.2205, -0.0854],\n",
      "        [-0.2552, -0.1068],\n",
      "        [-0.2989, -0.1944],\n",
      "        [-0.0295, -0.3146],\n",
      "        [-0.1540, -0.0397],\n",
      "        [-0.2067, -0.1911],\n",
      "        [-0.0725,  0.1996],\n",
      "        [-0.3558, -0.3276],\n",
      "        [-0.2353, -0.2867],\n",
      "        [-0.0963, -0.1815],\n",
      "        [-0.2948,  0.2724],\n",
      "        [-0.1875, -0.2943]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "max_val_acc = 0\n",
    "max_val_f1 = 0\n",
    "global_step = 0\n",
    "path = None\n",
    "for epoch in range(opt.num_epoch):\n",
    "    print('>' * 100)\n",
    "    print('epoch: {}'.format(epoch))\n",
    "    n_correct, n_total, loss_total = 0, 0, 0\n",
    "    # switch model to training mode\n",
    "    model.train()\n",
    "    for i_batch, sample_batched in enumerate(train_data_loader):\n",
    "        if i_batch <= 0 :\n",
    "            print (type(sample_batched))\n",
    "        global_step += 1\n",
    "        # clear gradient accumulators\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = [sample_batched[col].to(opt.device) for col in opt.inputs_cols]\n",
    "        if i_batch <= 0 :\n",
    "            print (\"=\" * 100)\n",
    "            print (\"inputs\")\n",
    "            print (len(inputs))\n",
    "            print (inputs)\n",
    "        outputs = model(inputs)\n",
    "        if i_batch <= 0 :\n",
    "            print (\"=\" * 100)\n",
    "            print (\"outputs\")\n",
    "            print (len(outputs))\n",
    "            print (outputs)\n",
    "\n",
    "        targets = sample_batched['polarity'].to(opt.device)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
    "        n_total += len(outputs)\n",
    "        loss_total += loss.item() * len(outputs)\n",
    "        if global_step % opt.log_step == 0:\n",
    "            train_acc = n_correct / n_total\n",
    "            train_loss = loss_total / n_total\n",
    "            logger.info('loss: {:.4f}, acc: {:.4f}'.format(train_loss, train_acc))\n",
    "\n",
    "    val_acc, val_f1, val_p, val_r = _evaluate_acc_f1(val_data_loader)\n",
    "    print('> val_acc: {:.4f}, val_f1: {:.4f}, val_p: {:.4f}, val_r: {:.4f}'.format(val_acc, val_f1, val_p, val_r))\n",
    "    #logger.info('> val_acc: {:.4f}, val_f1: {:.4f}, val_p: {:.4f}, val_r: {:.4f}'.format(val_acc, val_f1, val_p, val_r))\n",
    "    if val_acc > max_val_acc:\n",
    "        max_val_acc = val_acc\n",
    "        if not os.path.exists('state_dict'):\n",
    "            os.mkdir('state_dict')\n",
    "        path = 'state_dict/{0}_val_acc{2}'.format(opt.model_name, round(val_acc, 4))\n",
    "        torch.save(model.state_dict(), path)\n",
    "        logger.info('>> saved: {}'.format(path))        \n",
    "    if val_f1 > max_val_f1:\n",
    "        max_val_f1 = val_f1\n",
    "    print (\"max_val_acc:{}, max_val_f1:{}\".format(max_val_acc, max_val_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
